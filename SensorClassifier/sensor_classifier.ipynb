{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "RG1I4JZaq6-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as nnF\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "\n",
        "import os, sys, time, subprocess\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pickle\n",
        "import csv\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6VoOl9Pyq9BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive"
      ],
      "metadata": {
        "id": "FYzi_EwgrKl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount google drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# insert directory\n",
        "sys.path.insert(0, '/content/drive/MyDrive/CAS_AIArt/AIMovement/MotionClassifier')"
      ],
      "metadata": {
        "id": "3EyoFhamrT2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1NTK0a-FJNr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CAS_AIArt/AIMovement/MotionClassifier/\n",
        "%ls\n",
        "%pwd"
      ],
      "metadata": {
        "id": "OYZzjqQZx5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device"
      ],
      "metadata": {
        "id": "1KKLLb9mr-f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "id": "SSkXDzKFsACv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "f-HeHckBsfdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file_path = \"data\"\n",
        "data_file_extensions = [\".pkl\"]\n",
        "data_sensor_ids = [\"/accelerometer\", \"/gyroscope\"]\n",
        "data_window_length = 60\n",
        "data_window_offset = 1"
      ],
      "metadata": {
        "id": "gcOTh--SsiE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Settings"
      ],
      "metadata": {
        "id": "eNZ6_qWPtVFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = None # automatically determined\n",
        "hidden_dim = 32\n",
        "layer_count = 3\n",
        "class_count = None # automatically determined"
      ],
      "metadata": {
        "id": "-lyNeby3tZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Settings"
      ],
      "metadata": {
        "id": "JNWbLHjytdbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_percentage = 0.2\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "load_weights = False\n",
        "load_weights_epoch = 100"
      ],
      "metadata": {
        "id": "hwAWMW0WtfbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data Functions"
      ],
      "metadata": {
        "id": "BmoEkra4tiB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that filename has allowed extension\n",
        "def file_has_allowed_extension(filename, extensions):\n",
        "    return filename.lower().endswith(tuple(extensions))\n",
        "\n",
        "# load all recording files into a dictionary\n",
        "def load_recordings(data_file_path, extensions):\n",
        "\n",
        "    files_data = []\n",
        "\n",
        "    for root, _, file_names in sorted(os.walk(data_file_path, followlinks=True)):\n",
        "        for file_name in sorted(file_names):\n",
        "\n",
        "            file_path = root + \"/\" + file_name\n",
        "\n",
        "            if pathlib.Path(file_name).suffix not in data_file_extensions:\n",
        "                continue\n",
        "\n",
        "            file_path = root + \"/\" + file_name\n",
        "\n",
        "            print(\"file_path: \", file_path)\n",
        "\n",
        "            with open(file_path, \"rb\") as input_file:\n",
        "                file_data = pickle.load(input_file)\n",
        "\n",
        "                files_data.append(file_data)\n",
        "\n",
        "    return files_data\n",
        "\n",
        "# detect numner of classes\n",
        "def get_class_count(recording_files):\n",
        "    return len(set([ recording_file[\"class_id\"] for recording_file in recording_files ]))\n",
        "\n",
        "# filter and concatenate sensor values\n",
        "def process_recordings(data_files, data_sensor_ids):\n",
        "\n",
        "    recordings = []\n",
        "\n",
        "    for data_file in data_files:\n",
        "\n",
        "        recording = {}\n",
        "        recording[\"class_id\"] = data_file[\"class_id\"]\n",
        "        sensor_values_combined = []\n",
        "\n",
        "        min_data_count = None\n",
        "\n",
        "        for sensor_id in data_sensor_ids:\n",
        "\n",
        "            sensor_indices = [i for i, x in enumerate(data_file[\"sensor_ids\"]) if x == sensor_id]\n",
        "            sensor_values = [data_file[\"sensor_values\"][i] for i in sensor_indices]\n",
        "            sensor_values = np.array(sensor_values)\n",
        "\n",
        "            if min_data_count is None:\n",
        "                min_data_count = sensor_values.shape[0]\n",
        "            elif min_data_count > sensor_values.shape[0]:\n",
        "                min_data_count = sensor_values.shape[0]\n",
        "\n",
        "            sensor_values_combined.append(sensor_values)\n",
        "\n",
        "        # crop sensor values to smallest size\n",
        "        for i in range(len(sensor_values_combined)):\n",
        "            sensor_values_combined[i] = sensor_values_combined[i][:min_data_count, ...]\n",
        "\n",
        "        recording[\"sensor_values\"] = np.concatenate(sensor_values_combined, axis=1)\n",
        "\n",
        "        recordings.append(recording)\n",
        "\n",
        "    return recordings\n",
        "\n",
        "# calculate mean and std of sensor value\n",
        "def calc_norm_values(recording_data):\n",
        "\n",
        "    all_values = []\n",
        "\n",
        "    for data in recording_data:\n",
        "        all_values.append(data[\"sensor_values\"])\n",
        "\n",
        "    all_values = np.concatenate(all_values, axis=0)\n",
        "\n",
        "    values_mean = np.mean(all_values, axis=0)\n",
        "    values_std = np.std(all_values, axis=0)\n",
        "\n",
        "    return values_mean, values_std\n",
        "\n",
        "# create dataset\n",
        "def create_dataset(recording_data, window_length, window_offset):\n",
        "\n",
        "    sensor_data = []\n",
        "    class_labels = []\n",
        "\n",
        "    for recording in recording_data:\n",
        "\n",
        "        class_id = recording[\"class_id\"]\n",
        "        sensor_values = recording[\"sensor_values\"]\n",
        "        sensor_value_count = sensor_values.shape[0]\n",
        "\n",
        "        for eI in range(0, sensor_value_count - window_length, window_offset):\n",
        "            sensor_values_excerpt = sensor_values[eI:eI + window_length]\n",
        "\n",
        "            class_labels.append(class_id)\n",
        "            sensor_data.append(sensor_values_excerpt)\n",
        "\n",
        "    class_labels = np.array(class_labels, dtype=np.int64)\n",
        "    sensor_data = np.stack(sensor_data, axis=0).astype(np.float32)\n",
        "\n",
        "    return class_labels, sensor_data"
      ],
      "metadata": {
        "id": "FD4DkbWPti7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Process Sensor Data Recordings"
      ],
      "metadata": {
        "id": "vlZVntJ4ty9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recording_files = load_recordings(data_file_path, data_file_extensions)\n",
        "class_count = get_class_count(recording_files)\n",
        "recording_data = process_recordings(recording_files, data_sensor_ids)\n",
        "data_mean, data_std = calc_norm_values(recording_data)\n",
        "\n",
        "# save data mean and std\n",
        "with open(\"results/data/mean.pkl\", 'wb') as f:\n",
        "    pickle.dump(data_mean, f)\n",
        "with open(\"results/data/std.pkl\", 'wb') as f:\n",
        "    pickle.dump(data_std, f)\n"
      ],
      "metadata": {
        "id": "XKKrUwC5t0ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "1UW33rmJyaHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels, sensor_data = create_dataset(recording_data, data_window_length, data_window_offset)\n",
        "\n",
        "class SensorDataset(Dataset):\n",
        "\n",
        "    def __init__(self, class_labels, sensor_data):\n",
        "\n",
        "        self.class_labels = class_labels\n",
        "        self.sensor_data = sensor_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        x = self.sensor_data[index]\n",
        "        y = self.class_labels[index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_labels)\n",
        "\n",
        "\n",
        "dataset = SensorDataset(class_labels, sensor_data)\n",
        "\n",
        "item_x, item_y = dataset[0]\n",
        "print(\"item_x s \", item_x.shape)\n",
        "print(\"item_y s \", item_y.shape)\n",
        "\n",
        "# train test split\n",
        "dataset_size = len(dataset)\n",
        "test_size = int(test_percentage * dataset_size)\n",
        "train_size = dataset_size - test_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# create dataloader\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "batch_x, batch_y = next(iter(trainloader))\n",
        "\n",
        "print(\"batch_x s \", batch_x.shape)\n",
        "print(\"batch_y s \", batch_y.shape)"
      ],
      "metadata": {
        "id": "9BT1EDbQybLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "8sAt1JM_ygwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_count, class_count):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_count, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, class_count)\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x, (h, c) = self.rnn(x)\n",
        "        x = x[:, -1, :] # only last time step\n",
        "        x = self.fc(x)\n",
        "        y = self.sm(x)\n",
        "        return y\n",
        "\n",
        "input_dim = item_x.shape[-1]\n",
        "\n",
        "classifier = Classifier(input_dim, hidden_dim, layer_count, class_count)\n",
        "classifier.to(device)\n",
        "\n",
        "print(classifier)\n",
        "\n",
        "# test classifier\n",
        "batch = next(iter(trainloader))\n",
        "batch_x = batch[0].to(device)\n",
        "batch_y = batch[1].to(device)\n",
        "\n",
        "print(\"batch_x s \", batch_x.shape)\n",
        "print(\"batch_y s \", batch_y.shape)\n",
        "\n",
        "batch_yhat = classifier(batch_x)\n",
        "\n",
        "print(\"batch_yhat s \", batch_yhat.shape)\n",
        "\n",
        "# load model weights if available\n",
        "if load_weights and load_weights_epoch > 0:\n",
        "    classifier.load_state_dict(torch.load(\"results/weights/classifier_epoch_{}\".format(load_weights_epoch)))\n"
      ],
      "metadata": {
        "id": "IovSjUNuyiRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "kmF5YGicy4az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_loss = nn.NLLLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) # reduce the learning every 20 epochs by a factor of 10\n",
        "\n",
        "data_mean = torch.tensor(data_mean, dtype=torch.float32).reshape(1, 1, -1).to(device)\n",
        "data_std = torch.tensor(data_std, dtype=torch.float32).reshape(1, 1, -1).to(device)\n",
        "\n",
        "def train_step(batch_x, batch_y):\n",
        "\n",
        "    batch_x_norm = (batch_x - data_mean) / data_std\n",
        "\n",
        "    batch_yhat = classifier(batch_x_norm)\n",
        "    _loss = class_loss(batch_yhat, batch_y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    _loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return _loss\n",
        "\n",
        "def test_step(batch_x, batch_y):\n",
        "\n",
        "    batch_x_norm = (batch_x - data_mean) / data_std\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_yhat = classifier(batch_x_norm)\n",
        "        _loss = class_loss(batch_yhat, batch_y)\n",
        "\n",
        "    return _loss\n",
        "\n",
        "def test_model(data_loader):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "\n",
        "            batch_x, batch_y = data\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            batch_x_norm = (batch_x - data_mean) / data_std\n",
        "\n",
        "            outputs = classifier(batch_x_norm)\n",
        "\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        return correct / total\n",
        "\n",
        "def train(train_dataloader, test_dataloader, epochs):\n",
        "\n",
        "    loss_history = {}\n",
        "    loss_history[\"train\"] = []\n",
        "    loss_history[\"test\"] = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss_per_epoch = []\n",
        "\n",
        "        for train_data in train_dataloader:\n",
        "\n",
        "            batch_x, batch_y = train_data\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            _train_loss = train_step(batch_x, batch_y)\n",
        "            _train_loss = _train_loss.detach().cpu().numpy()\n",
        "            train_loss_per_epoch.append(_train_loss)\n",
        "\n",
        "        train_loss_per_epoch = np.mean(np.array(train_loss_per_epoch))\n",
        "\n",
        "        loss_history[\"train\"].append(train_loss_per_epoch)\n",
        "\n",
        "\n",
        "        test_loss_per_epoch = []\n",
        "\n",
        "        for test_data in test_dataloader:\n",
        "\n",
        "            batch_x, batch_y = test_data\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            _test_loss = test_step(batch_x, batch_y)\n",
        "            _test_loss = _test_loss.detach().cpu().numpy()\n",
        "            test_loss_per_epoch.append(_test_loss)\n",
        "\n",
        "        test_loss_per_epoch = np.mean(np.array(test_loss_per_epoch))\n",
        "\n",
        "        loss_history[\"test\"].append(test_loss_per_epoch)\n",
        "\n",
        "\n",
        "        train_correct = test_model(train_dataloader)\n",
        "        test_correct = test_model(test_dataloader)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print ('epoch {} : train: loss {:01.4f} corr {:01.2f} test: loss {:01.4f} correct {:01.2f} time {:01.2f}'.format(epoch + 1, train_loss_per_epoch, train_correct * 100, test_loss_per_epoch, test_correct * 100, time.time()-start))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "# fit model\n",
        "loss_history = train(trainloader, testloader, epochs)"
      ],
      "metadata": {
        "id": "-H4dHdHfy32t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Training"
      ],
      "metadata": {
        "id": "JjscEtayzJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_loss_as_csv(loss_history, csv_file_name):\n",
        "    with open(csv_file_name, 'w') as csv_file:\n",
        "        csv_columns = list(loss_history.keys())\n",
        "        csv_row_count = len(loss_history[csv_columns[0]])\n",
        "\n",
        "\n",
        "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        for row in range(csv_row_count):\n",
        "\n",
        "            csv_row = {}\n",
        "\n",
        "            for key in loss_history.keys():\n",
        "                csv_row[key] = loss_history[key][row]\n",
        "\n",
        "            csv_writer.writerow(csv_row)\n",
        "\n",
        "\n",
        "def save_loss_as_image(loss_history, image_file_name):\n",
        "    keys = list(loss_history.keys())\n",
        "    epochs = len(loss_history[keys[0]])\n",
        "\n",
        "    for key in keys:\n",
        "        plt.plot(range(epochs), loss_history[key], label=key)\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(image_file_name)\n",
        "    plt.show()\n",
        "\n",
        "save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n",
        "save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))\n",
        "\n",
        "# save trained model\n",
        "PATH = 'results/weights/classifier_weights_epoch_{}.pth'.format(epochs)\n",
        "torch.save(classifier.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "dAZ9NVHXzLO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "MKCtjv4lzx_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_x, batch_y = next(iter(testloader))\n",
        "batch_yhat = classifier(batch_x.to(device))\n",
        "_, pred_labels = torch.max(batch_yhat, 1)\n",
        "\n",
        "for i in range(batch_size):\n",
        "    print(\"motion {} pred class {} true class {}\".format(i, pred_labels[i], batch_y[i]))"
      ],
      "metadata": {
        "id": "KJSxfF9hzzlv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}